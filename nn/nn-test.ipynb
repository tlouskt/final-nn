{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea7db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This is a class that generates a fully-connected neural network.\n",
    "\n",
    "    Parameters:\n",
    "        nn_arch: List[Dict[str, float]]\n",
    "            A list of dictionaries describing the layers of the neural network.\n",
    "            e.g. [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 8, 'activation:': 'sigmoid'}]\n",
    "            will generate a two-layer deep network with an input dimension of 64, a 32 dimension hidden layer, and an 8 dimensional output.\n",
    "        lr: float\n",
    "            Learning rate (alpha).\n",
    "        seed: int\n",
    "            Random seed to ensure reproducibility.\n",
    "        batch_size: int\n",
    "            Size of mini-batches used for training.\n",
    "        epochs: int\n",
    "            Max number of epochs for training.\n",
    "        loss_function: str\n",
    "            Name of loss function.\n",
    "\n",
    "    Attributes:\n",
    "        arch: list of dicts\n",
    "            (see nn_arch above)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #got TypeError, changed Union(int, str) to Union[int, str]\n",
    "        nn_arch: List[Dict[str, Union[int, str]]],\n",
    "        lr: float,\n",
    "        seed: int,\n",
    "        batch_size: int,\n",
    "        epochs: int,\n",
    "        loss_function: str\n",
    "    ):\n",
    "\n",
    "        # Save architecture\n",
    "        self.arch = nn_arch\n",
    "\n",
    "        # Save hyperparameters\n",
    "        self._lr = lr\n",
    "        self._seed = seed\n",
    "        self._epochs = epochs\n",
    "        self._loss_func = loss_function\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # Initialize the parameter dictionary for use in training\n",
    "        self._param_dict = self._init_params()\n",
    "\n",
    "    def _init_params(self) -> Dict[str, ArrayLike]:\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY THIS METHOD! IT IS ALREADY COMPLETE!\n",
    "\n",
    "        This method generates the parameter matrices for all layers of\n",
    "        the neural network. This function returns the param_dict after\n",
    "        initialization.\n",
    "\n",
    "        Returns:\n",
    "            param_dict: Dict[str, ArrayLike]\n",
    "                Dictionary of parameters in neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Seed NumPy\n",
    "        np.random.seed(self._seed)\n",
    "\n",
    "        # Define parameter dictionary\n",
    "        param_dict = {}\n",
    "\n",
    "        # Initialize each layer's weight matrices (W) and bias matrices (b)\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            input_dim = layer['input_dim']\n",
    "            output_dim = layer['output_dim']\n",
    "            param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "\n",
    "        return param_dict\n",
    "\n",
    "    def _single_forward(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        activation: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single forward pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            activation: str\n",
    "                Name of activation function for current layer.\n",
    "\n",
    "        Returns:\n",
    "            A_curr: ArrayLike\n",
    "                Current layer activation matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transformed matrix.\n",
    "        \"\"\"\n",
    "        #Apply linear transofrm to previous activation layer (z(l+1) = w(l)a(l) + b(l))\n",
    "        Z_curr = np.dot(A_prev, W_curr.T) + b_curr.T\n",
    "\n",
    "        #Apply activation function to activation matrix for next layer\n",
    "        if activation == \"relu\":\n",
    "            A_curr = self._relu(Z_curr)\n",
    "        elif activation == \"sigmoid\":\n",
    "            A_curr = self._sigmoid(Z_curr)\n",
    "        else:\n",
    "            raise ValueError(\"Activation function is neither relu or sigmoid\")\n",
    "        \n",
    "        return A_curr, Z_curr\n",
    "\n",
    "    def forward(self, X: ArrayLike) -> Tuple[ArrayLike, Dict[str, ArrayLike]]:\n",
    "        \"\"\"\n",
    "        This method is responsible for one forward pass of the entire neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input matrix with shape [batch_size, features].\n",
    "\n",
    "        Returns:\n",
    "            output: ArrayLike\n",
    "                Output of forward pass.\n",
    "            cache: Dict[str, ArrayLike]:\n",
    "                Dictionary storing Z and A matrices from `_single_forward` for use in backprop.\n",
    "        \"\"\"\n",
    "        #Initialize empty cache dict, previous activaiton matrix, and add it to cache\n",
    "        cache = {}\n",
    "        cache['A0'] = X\n",
    "        A_prev = X\n",
    "\n",
    "        #Iterate through network and pull out weights and bias from each layer\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "\n",
    "            #Get current weights, bias, and activation\n",
    "            W_curr = self._param_dict['W' + str(layer_idx)]\n",
    "            b_curr = self._param_dict['b' + str(layer_idx)]\n",
    "            activation = layer['activation']\n",
    "\n",
    "            #Single forward pass to get activation matrix and linearly transformed matrix\n",
    "            A_curr, Z_curr = self._single_forward(W_curr, b_curr, A_prev, activation)\n",
    "            \n",
    "            #Store A_curr matrix and Z_curr matrix in cache\n",
    "            cache[\"A\" + str(layer_idx)] = A_curr\n",
    "            cache[\"Z\" + str(layer_idx)] = Z_curr\n",
    "\n",
    "            #Set A_curr to be A_prev for next loop\n",
    "            A_prev = A_curr\n",
    "        \n",
    "        #return final activation matrix and cache\n",
    "        return A_prev, cache\n",
    "\n",
    "    def _single_backprop(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        Z_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        dA_curr: ArrayLike,\n",
    "        activation_curr: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single backprop pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transform matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            dA_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer activation matrix.\n",
    "            activation_curr: str\n",
    "                Name of activation function of layer.\n",
    "\n",
    "        Returns:\n",
    "            dA_prev: ArrayLike\n",
    "                Partial derivative of loss function with respect to previous layer activation matrix.\n",
    "            dW_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer weight matrix.\n",
    "            db_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer bias matrix.\n",
    "        \"\"\"\n",
    "        #Backprop based on current layer activation\n",
    "        if activation_curr == \"relu\":\n",
    "            dZ_curr = self._relu_backprop(dA_curr, Z_curr)\n",
    "        elif activation_curr == \"sigmoid\":\n",
    "            dZ_curr = self._sigmoid_backprop(dA_curr, Z_curr)\n",
    "        else:\n",
    "            raise ValueError(\"Activation function is neither relu nor sigmoid\")\n",
    "        \n",
    "        #number observations m to divide 1/m\n",
    "        #m = A_prev.shape[1]\n",
    "\n",
    "\n",
    "        #Calculate partial derivatives\n",
    "        dA_prev = np.dot(dZ_curr, W_curr)\n",
    "        dW_curr = np.dot(dZ_curr.T, A_prev)\n",
    "        db_curr = np.sum(dZ_curr, axis=0).reshape(b_curr.shape)\n",
    "\n",
    "        return dA_prev, dW_curr, db_curr\n",
    "\n",
    "\n",
    "    def backprop(self, y: ArrayLike, y_hat: ArrayLike, cache: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This method is responsible for the backprop of the whole fully connected neural network.\n",
    "\n",
    "        Args:\n",
    "            y (array-like):\n",
    "                Ground truth labels.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output values.\n",
    "            cache: Dict[str, ArrayLike]\n",
    "                Dictionary containing the information about the\n",
    "                most recent forward pass, specifically A and Z matrices.\n",
    "\n",
    "        Returns:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from this pass of backprop.\n",
    "        \"\"\"\n",
    "        #init grad_dict\n",
    "        grad_dict = {}\n",
    "\n",
    "        #decide what the loss is to backprop from on outermost layer(loss)\n",
    "        if self._loss_func == \"bce\":\n",
    "            dA_curr = self._binary_cross_entropy_backprop(y, y_hat)\n",
    "        elif self._loss_func == \"mse\":\n",
    "            dA_curr = self._mean_squared_error_backprop(y, y_hat)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss function. Not bce or mse\")\n",
    "        \n",
    "        #propagate backwards through the layers, adjust weights via gradient descent\n",
    "        for idx, layer in reversed(list(enumerate(self.arch))):\n",
    "            layer_idx = idx + 1\n",
    "            #get  weights, bias, activation from layer\n",
    "            activation = layer[\"activation\"]\n",
    "            W_curr = self._param_dict[\"W\" + str(layer_idx)]\n",
    "            b_curr = self._param_dict[\"b\" + str(layer_idx)]\n",
    "\n",
    "            #get Z and previous A matrix from cache\n",
    "            A_prev = cache[\"A\" + str(idx)]\n",
    "            Z_curr = cache[\"Z\" + str(layer_idx)]\n",
    "\n",
    "            #run backprop\n",
    "            dA_prev, dW_curr, db_curr = self._single_backprop(W_curr, b_curr, Z_curr, A_prev, dA_curr, activation)\n",
    "\n",
    "            #store gradient values\n",
    "            grad_dict[\"dW\" + str(layer_idx)] = dW_curr\n",
    "            grad_dict[\"db\" + str(layer_idx)] = db_curr\n",
    "\n",
    "            #update activation matrix\n",
    "            dA_curr = dA_prev\n",
    "\n",
    "        return grad_dict\n",
    "\n",
    "    def _update_params(self, grad_dict: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This function updates the parameters in the neural network after backprop. This function\n",
    "        only modifies internal attributes and does not return anything\n",
    "\n",
    "        Args:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from most recent round of backprop.\n",
    "        \"\"\"\n",
    "        #Update weights and bias\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            #curr W val - (lr *  gradW)\n",
    "            self._param_dict[\"W\" + str(layer_idx)] = self._param_dict[\"W\" + str(layer_idx)] - (self._lr * grad_dict[\"dW\" + str(layer_idx)])\n",
    "            #curr b val - (lr *  gradb)\n",
    "            self._param_dict[\"b\" + str(layer_idx)] = self._param_dict[\"b\" + str(layer_idx)] - (self._lr * grad_dict[\"db\" + str(layer_idx)])\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: ArrayLike,\n",
    "        y_train: ArrayLike,\n",
    "        X_val: ArrayLike,\n",
    "        y_val: ArrayLike\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        This function trains the neural network by backpropagation for the number of epochs defined at\n",
    "        the initialization of this class instance.\n",
    "\n",
    "        Args:\n",
    "            X_train: ArrayLike\n",
    "                Input features of training set.\n",
    "            y_train: ArrayLike\n",
    "                Labels for training set.\n",
    "            X_val: ArrayLike\n",
    "                Input features of validation set.\n",
    "            y_val: ArrayLike\n",
    "                Labels for validation set.\n",
    "\n",
    "        Returns:\n",
    "            per_epoch_loss_train: List[float]\n",
    "                List of per epoch loss for training set.\n",
    "            per_epoch_loss_val: List[float]\n",
    "                List of per epoch loss for validation set.\n",
    "        \"\"\"\n",
    "        #init lists to store losses\n",
    "        per_epoch_train_loss = []\n",
    "        per_epoch_val_loss = []\n",
    "\n",
    "        num_batches = np.ceil(X_train.shape[0] / self._batch_size)\n",
    "\n",
    "        for epoch in range(self._epochs):\n",
    "            #shuffle training data\n",
    "            shuffle = np.random.permutation(X_train.shape[0])\n",
    "            X_train_shuffle = X_train[shuffle]\n",
    "            y_train_shuffle = y_train[shuffle]\n",
    "            #split training into batches\n",
    "            X_batch = np.array_split(X_train_shuffle, num_batches)\n",
    "            y_batch = np.array_split(y_train_shuffle, num_batches)\n",
    "\n",
    "            #init list to keep track of batch train loss per epoch\n",
    "            batch_train_loss = []\n",
    "            #iterate through batches and do forward and backward passes\n",
    "            for X_train_batch, y_train_batch in zip(X_batch, y_batch):\n",
    "                #forward pass\n",
    "                y_hat_train, cache_train = self.forward(X_train_batch)\n",
    "                #calculate losses and add to loss list\n",
    "                if self._loss_func == \"bce\":\n",
    "                    batch_train_loss.append(self._binary_cross_entropy(y_train_batch, y_hat_train))\n",
    "                else:\n",
    "                    self._loss_func == \"mse\"\n",
    "                    batch_train_loss.append(self._mean_squared_error(y_train_batch, y_hat_train))\n",
    "                #backpropagation and update params\n",
    "                grad_dict = self.backprop(y_train_batch, y_hat_train, cache_train)\n",
    "                self._update_params(grad_dict)\n",
    "            #get average loss for current epoch and add to list\n",
    "            per_epoch_train_loss.append(np.mean(batch_train_loss))\n",
    "\n",
    "            #get validation loss for current epoch\n",
    "            y_hat_val = self.predict(X_val)\n",
    "            if self._loss_func == \"bce\":\n",
    "                val_loss = self._binary_cross_entropy(y_val, y_hat_val)\n",
    "            else:\n",
    "                self._loss_func == \"mse\"\n",
    "                val_loss = self._mean_squared_error(y_val, y_hat_val)\n",
    "            \n",
    "            per_epoch_val_loss.append(val_loss)\n",
    "        \n",
    "        return per_epoch_train_loss, per_epoch_val_loss\n",
    "\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        This function returns the prediction of the neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            y_hat: ArrayLike\n",
    "                Prediction from the model.\n",
    "        \"\"\"\n",
    "        #forward pass\n",
    "        y_hat, _ = self.forward(X)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "    def _sigmoid(self, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        nl_transform = 1 / (1 + np.exp(-Z))\n",
    "\n",
    "        return nl_transform\n",
    "\n",
    "    def _sigmoid_backprop(self, dA: ArrayLike, Z: ArrayLike):\n",
    "        \"\"\"\n",
    "        Sigmoid derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        #f'(z)=f(z)(1âˆ’f(z)\n",
    "        sigmoid_Z = self._sigmoid(Z)\n",
    "        dZ = sigmoid_Z * (1-sigmoid_Z)\n",
    "\n",
    "        return dZ * dA #chain rule\n",
    "\n",
    "    def _relu(self, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        nl_transform = np.maximum(0,Z)\n",
    "\n",
    "        return nl_transform\n",
    "\n",
    "    def _relu_backprop(self, dA: ArrayLike, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        ReLU derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        dZ = np.multiply(dA, np.where(Z > 0, 1, 0))\n",
    "\n",
    "        return dZ\n",
    "        \n",
    "\n",
    "    def _binary_cross_entropy(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss over mini-batch.\n",
    "        \"\"\"\n",
    "        #(-1/N)(sum(y_i * log*y_hat) + ((1-y_i))*(log(1-y_hat)))\n",
    "\n",
    "        #avoid divide by 0 error by clipping y_hat values\n",
    "        y_hat = np.clip(y_hat, 1e-6, 1 - 1e-6)\n",
    "\n",
    "        bce_loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "        return bce_loss\n",
    "\n",
    "    def _binary_cross_entropy_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        #avoid divide by 0 error by clipping y_hat values\n",
    "        y_hat = np.clip(y_hat, 1e-6, 1 - 1e-6)\n",
    "\n",
    "        #chain rule \n",
    "        dA = ((1-y)/(1-y_hat)) - (y/y_hat)\n",
    "        \n",
    "        return dA\n",
    "\n",
    "\n",
    "    def _mean_squared_error(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Mean squared error loss.\n",
    "\n",
    "        Args:\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss of mini-batch.\n",
    "        \"\"\"\n",
    "        mse_loss  = np.mean((y - y_hat)**2)\n",
    "        return mse_loss\n",
    "\n",
    "    def _mean_squared_error_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Mean square error loss derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        # -2/N * (y - y_hat)\n",
    "        dA = (-2 * (y - y_hat)) / len(y)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "344c170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b22da74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load digits dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0a8b86d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (360, 64))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30906dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = [{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'},\n",
    "       {'input_dim': 16, 'output_dim': 64, 'activation': 'relu'}]\n",
    "# initialize autoencoder\n",
    "model = NeuralNetwork(arch, lr = 0.0001, seed = 42, batch_size = 50, epochs = 500, loss_function = \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8e37c13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train autoencoder on the training data\n",
    "train_loss, test_loss = model.fit(X_train, X_train, X_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b15df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average Training Error: 6.064917599905919\n",
      "average Validation Error: 6.416368347717475\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, label=\"train\")\n",
    "plt.plot(test_loss, label=\"val\")\n",
    "plt.title(\"Training and Validation Losses Per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "print(f'average Training Error: {np.mean(train_loss)}')\n",
    "print(f'average Validation Error: {np.mean(test_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f1f9f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average prediction error: 5.884491004812239\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred_err = mean_squared_error(X_test, pred)\n",
    "print(\"Average prediction error: \" + str(pred_err))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 10))\n",
    "ax[0].matshow(X_test[3].reshape((8, 8)))\n",
    "ax[0].set_title(\"True\")\n",
    "ax[1].matshow(pred[3].reshape((8, 8)))\n",
    "ax[1].set_title(\"Predict\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f34dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
